1. RPC
   RPC框架通过提供一种透明的调用机制,让使用者不必显式区分本地调用和远程调用,从而让构建分布式计算or应用更加方便
   RPC调用分为同步和异步调用两种,同步调用指客户端发起调用后,等待执行结果的返回,异步调用指客户端不用等待执行返回,可以通过回调等方式获取结果
2. 正则表达式描述了一种字符串匹配的模式
   正则中的非打印字符:\f 换页符; \n 换行符; \r 回车符; \s 任意空白字符,包括空格,制表符,换页符等; \S 任意非空白符,等价于[^\f\n\r\t\v];
   \t 制表符; \v 垂直制表符
   正则中的特殊字符:$ 字符串的结尾位置; ^ 字符串的开始位置; () 子表达式的开始和结束位置,在中括号表达式中表示不在该集合中;
   * 匹配前面的子表达式零次或多次; + 匹配前面的子表达式一次或多次; ? 匹配前面的子表达式零次或一次; . 匹配除换行符\n以为的任意一个字符; 
   [] 中括号表达式; | 两项之间的一个
   正则中的限定符,表示表达式中的一个组件,必须要出现多少次才能满足匹配,除了*/+/?之外,还包括{n},{n,},{n,m}
   注: * + 限定符都是贪婪的,它们会尽可能多的匹配字符,只有在它们后面加上'?',就可以实现非贪婪或者最小匹配
   正则中的定位符,用来描述字符串或单词的边界, ^/$ 表示字符串的开始和结束; \b 描述单词的前/后边界; \B 非单词边界
3. 需要在另外一台电脑上checkout一些文件,输入地址&账号&密码后,TortoiseSVN提示:
   Unable to connect to a repository atu URL "xxxxxxx" Access to '/svn/xxx' forbidden
   在网上查了一下,SVN --> Settings --> Saved Data --> 几个相关的clear按钮点一下,之后重新checkout,bingo...
   参考:https://blog.csdn.net/wx_lanyu/article/details/84207303
4. 记录两个sublime的快捷键
    一个是常用的Ctrl+D,选中文本,当有多个的时候,或者想只选中其中的几个的时候,常用的是该快捷键,如果想对所有文本进行操作,一个一个选显然太low,
    可替代的就是先选中文本,再Alt+F3,即可选中全部文本;
    另一个是列操作,常用的是Ctrl+Alt+上下箭头,后边遇到个情况,需要多从mysql中导出的1w多条记录进行列操作,这时候一直按箭头显然也不现实,
    可替代的是先Ctrl+A选中所有的记录,再Ctrl+Shift+L ('L'大小写均可)
5. 从系统架构来看,目前的服务器可分为三大体系结构,分别是对称多处理器(SMP),非一致存储访问(NUMA)以及海量并行处理(MPP)
   SMP(Symmetric Multi Processor),所谓的对称是指多个cpu对称工作,没有主次和从属关系,因此SMP也被称为一直存储访问结构(UMA,Uniform Memory Access)
   对称多处理器系统内有很多紧耦合多处理器,所有的CPU共享总线,内存,I/O等系统资源,因此该系统最大的特点就是共享所有的资源
   cpu之间没有区别,平等的访问内存,外设,共用一个操作系统,操作系统管理一个队列,每个处理器依次处理队列中的进程,如果两个处理器同时访问一个资源
   (同一段内存地址),由硬/软件的锁机制处理资源争用问题,对SMP的扩展方式包括增加内存和cpu,更换更快的cpu,扩充I/O;
   NUMA(Non-uniform Memory Access)非一致存储访问架构,是一种为多处理器电脑设计的内存架构,内存访问时间取决于处理的内存位置,NUMA服务器的基本特征是
   具有多个cpu模块,每个cpu模块又有多个cpu组成,并且具有独立的本地内存和I/O槽口,由于节点之间可以通过互联模块(crossbar switch)进行连接和信息交互,
   因此每个cpu都可以访问整个系统内存,显然访问本地内存的速度远远高于访问其它节点内存的速度,这也就是非一致的由来,NUMA的主要缺陷也在于此,当添加多个
   cpu模块时系统性能无法线性增加;
   MMP(Massive Parallel Processing)与NUMA不同,MPP提供了另外一种扩展系统的方式,其基本结构是由多个SMP服务器(称之为节点),通过节点互联网络连接而成,
   每个SMP节点只访问自己的本地资源,是一种完全无共享的结构,在MPP系统中,每个SMP节点也可以运行自己的内存,总线,操作系统和数据库等,
   与NUMA不同的是,MPP不存在访问其它节点内存的问题,节点之间的信息交互是通过互联网络实现的,这个过程一般称之为数据重分配(Data Redistribution)
   参考:http://www.elecfans.com/baike/computer/fuwuqi/20171023568144.html
6. Protocol buff编码格式及sxxx的zigzag编码:
   https://www.cnblogs.com/cobbliu/archive/2013/03/02/2940074.html
   https://izualzhy.cn/protobuf-encode-varint-and-zigzag#4-zigzag%E7%BC%96%E7%A0%81
   protocol buff的数据类型:
   VARIINT 可变长度整型,该类型数据使用varint编码对所传入的数据进行压缩存储,int32/64,uint32/64,sint32/64,bool,enum属该类型
   FIXED32/64 固定长度整型,不会对传入的数据进行varint压缩,只存储原始数据,fixed32,sfixed32,float属FIXED32,fixed64,sfixed64,double属FIXED64
   LENGTH_DELIMITED 长度界定型数据,主要针对string,bytes,embedded messages,packed repeated field,简言之就是针对string类型,repeated和嵌套
   类型,对这些类型数据进行编码时需要保存它们的长度信息
   START_GROUP 组的开始标志,组也可以是repeated或嵌套类型
   END_GROUP 组的结束标志,其余同上
7. 待挖掘的: https://www.jianshu.com/u/38eb16b24cb9 
8. SIMD(Single Instruction Multiple Data)
9. 栈帧(Stack Frame)
    在数据结构中,栈是限定只能在表尾进行插入或删除操作的线性表,是按照后进先出存储数据的一种数据结构,在计算机系统中,栈也称为栈内存,是一块动态内存区域,
    栈用于维护函数调用的上下文,包括函数内部的局部变量,函数调用,函数参数值及返回值等,栈由系统分配,存储地址连续且有上限,因此存在栈溢出现象
    每一次函数的调用都会在调用栈(call stack)上维护一个独立的栈帧,每个栈帧包括以下几个部分:
    a> 函数的返回地址和参数;
    b> 临时变量,包括函数内部的非静态局部变量和编译器生成的其它临时变量
    c> 函数调用的上下文,一个函数的栈帧由ebp和esp这两个寄存器来划定范围,ebp指向当前栈帧底部,ebp寄存器又称为帧指针,esp指向栈帧顶部,
       esp寄存器又称为栈指针,函数的调用过程中,有调用者(caller)和被调用者(callee),调用者需要知道被调用者的返回值,被调用者需要知道传入的参数和
       返回地址
    函数的调用过程:
    a> 参数入栈,将参数从右向左依次压入系统栈中
    b> 返回地址入栈,将当前代码区调用指令的下一条指令地址压入栈中
    c> 代码区跳转,处理器从当前代码区跳转到被调用函数的入口处
    d> 栈帧调整,具体包括:
        1> 将当前栈帧的ebp入栈
        2> 将esp装入ebp,切换当前栈帧到新栈帧(ebp,esp这两个寄存器指定了当前栈帧)
        3> 给新栈帧分配空间,通常是sub指令(栈内存地址从高到低,当前站定esp执行sub xxx,则往低地址移动)
    e> 在新栈帧执行当前被调用的函数
    函数的返回过程:
    a> 保存返回值,通常将返回值保存在寄存器EAX中
    b> 弹出当前栈帧,恢复调用函数的栈帧,具体包括:
        1> 在堆栈平衡的基础上,给esp加上栈帧大小(执行函数调用时栈帧调整第2步的相反操作),降低栈顶,回收当前栈空间
        2> 将当前栈顶保存的前栈帧ebp弹出并装入ebp寄存器
        3> 将当前栈顶的返回地址(要执行的下一条指令地址)弹给EIP寄存器(此时esp又回到了函数调用前的位置)
    c> 跳转,按照函数返回地址跳回母函数中继续执行
    esp(extended stack pointer)栈指针寄存器,该指针永远指向系统栈最上面一个栈帧(当前运行函数的栈帧)的顶部
    ebp(extended base pointer)基址指针寄存器,同esp,永远指向系统栈最上面一个栈帧的底部
    eip(extended instruction pointer)指令寄存器,该指针永远指向下一条待执行的指令地址,控制了eip就控制了进程,eip的内容决定了cpu会去哪里执行命令
    参考:
      https://www.cnblogs.com/dwlsxj/p/Stack.html
      https://blog.csdn.net/SKI_12/article/details/80554677
10. 必看书籍:深入理解计算机系统,程序员的自我修养
11. 经典GC(garbage collection)算法
    a> 引用计数(reference counting),每个单元维护一个域,保存其它单元指向它的引用数量,当引用数量为0时,将它回收,引用计数的有点是:渐进式,内存管理
       与用户程序的执行交织在一起,内存管理的开销分散到整个程序运行之中(c++的share_ptr使用的就是引用计数),不像标记-清除算法需要STW(stop the 
       world,GC时挂起用户程序); 算法易于实现; 内存单元很快被回收,不像其它算法,堆被耗尽或者达到某个阈值时才会进行垃圾回收
       引用计数算法的缺点是:原始的引用计数不能处理循环引用,针对该问题,有给出的解决方案,比如强引用; 维护引用计数,降低程序运行效率,内存的更新删除等
       操作,都需要维护相关内存单元的引用计数; 内存单元池实现方式(free_list)不是cache-friendly,会导致频繁的cache-miss,降低运行效率
    b> 标记-清除(mark sweep),是第一种自动内存管理,基于追踪的垃圾收集算法,内存单元并不会在变成垃圾内存时立即回收,而是保持不可达状态,直到达到某个
       阈值或者到固定时间,此时系统挂起用户程序(STW),转而执行垃圾回收程序,回收程序对所有存活内存单元进行一次全局扫描,确定哪些可以回收,算法分为两步:
       标记阶段,扫描并确认所有可回收的内存单元,清除阶段,将垃圾内存单元回收,标记-清除算法的优点是避免了引用计数算法的缺点(不能处理循环引用,需要维护
       引用计数),缺点是需要STW
    c> 三色标记算法(tricolor mark-sweep algorithm),三色标记算法是对描述阶段的改进,原理如下:
       初始所有内存对象均为白色; 从根对象(全局对象,线程栈)出发扫描所有可达对象,标记为灰色,放入待处理队列; 从队列取出灰色对象,将其引用对象标记为
       灰色,并放入队列,找到所有引用对象或无引用对象后,将当前灰色对象标记为黑色; 重复上一步骤,直至队列为空,此时白色对象即为垃圾内存,进行回收
       Go的三次标记算法流程:
       初始状态下,进程空间里每个内存对象都是白色,扫描开始前,先做一次短暂的STW,将扫描任务作为多个并发的goroutine立即入队给调度器,第一轮先扫描所有
       可达的内存对象,标记灰色并放入处理队列,第二轮start the world,取上一步的灰色队列,扫描其引用对象加入队列,并置黑,直至队列为空,第三轮再次STW,
       对上一个处理过程中新增/更新的内存进行标记(使用了操作系统的写屏障,write barrier),mark完毕,start the world,并发执行sweep操作
       go的gc需要注意mark有两个过程:
         首先从root开始遍历,root包括全局指针和goroutine栈上的指针,标记为灰色,并遍历灰色队列;
         re-scan全局指针和栈,因为mark和用户程序是并行的(开始mark时有一个极短的STW),所以在第一次遍历过程中可能会有新的内存对象或内存对象的更新,
         此时需要通过写屏障记录下来,re-scan再检查一遍;
       STW也有两个过程:
         第一次上文有提到过,就是在GC要开始的时候,主要做一些准备工作,包括:enable write barrier;
         第二次就是上文的re-scan过程,这个过程如果不STW,则有可能又会产生新的对象生成or更新,这样会导致无止尽的mark,所以此时STW,挂起用户程序
         Go的垃圾回收还存在一种情况,扫描的速度跟不上回收的速度,这将导致内存膨胀,比如多核系统中,3个核执行用户程序,1个核执行gc,这样就有可能出现上述
         情况,解决办法是,当Go发现回收速度跟不上分配速度的时候,会把其它运行用户程序的核抢过来做垃圾回收,造成事实上的STW,此时可全力进行垃圾回收
    d> 节点复制,也是基于追踪的算法,将整个堆等分为两个半区(semi-space),一个是Fromspace,一个是Tospace,节点复制会切换两个半区的角色,初始时,所有活动
       对象都在Fromspace,gc开始时,扫描Fromspace,将活动(被引用)对象复制到Tospace,遍历完Fromspace之后,所有活动对象都在Tospace,释放内存时,直接释放
       Fromspace即可,切换Fromspace/Tospace角色,用户程序可以继续运行,算法优点是:所有存活的数据结构,都缩并的排列在Tospace底部,这样不存在内存碎片的
       问题; 获取新内存可以简单的通过递增自由空间指针来获得,算法最大的缺点是,内存得不到充分利用,总有一半空间处于浪费状态 
    e> 分代收集(generational garbage collection),基于追踪的垃圾回收算法存在一个问题,在生命周期较长的对象上浪费时间(生命周期较长的对象不需要频繁
       扫描),同时,内存分配存在一个事实,"most object die young"(大多数对象会很快被回收),基于这两点,分代垃圾回收将内存对象按生命周期长短存放到堆
       的两个(或者多个)区域上,这个区域就是分代,新对象就在新生代中分配,如果gc扫描发现该对象已被扫描过,则将该对对象移到老年代,这个过程叫promote,
       随着不断promote,新生代在堆中的占比不会特别大,gc回收主要集中在新生代,回收的效率也会比较高,STW的时间更短,新生代的回收频率要比老年代高
       erlang采用该算法进行垃圾回收,同时基于erlang线程避免了STW,erlang:garbage_collect/0/1/2对当前or指定进程进行垃圾回收,可以通过环境变量
       ERL_FULLSWEEP_AFTER及erlang:system_flag(fullsweep_after,Number)设置Number次浅扫描后,触发深度扫描
12. HTTP缓存:https://mp.weixin.qq.com/s?__biz=MzUzMjk0ODI0OA==&mid=2247483754&idx=1&sn=5882213a4c48c6938ce5029f9b267a27&chksm=faaa351dcdddbc0bd3a6eb69f9e1fe6711a39320737c1db6035296037fc8a9b677b4c98a62dd&token=1212449367&lang=zh_CN&scene=21#wechat_redirect
13. DMA(Direct Memory Access)直接内存存取,允许不同速度的硬件来沟通,而不需要依赖cpu参与,DMA并不见得速度有多快,只是它不需要cpu参与,所以在DMA拷贝
    数据的时候,cpu可以做别的事,当然两者也有有所影响,毕竟都需要使用系统的数据总线传输数据,DMA将数据从一个地址空间复制到另外一个地址空间,cpu初始化
    这个拷贝动作,拷贝本身由DMA控制器来实行和完成,在实现DMA传输时,是由DMA控制器直接掌管总线,因此存在总线控制权转移的问题,即DMA拷贝前,cpu将总线控制
    权交给DMA控制器,拷贝结束后,DMA控制器再把控制权交还给cpu,一个完整的DMA拷贝过程包括:DMA请求,DMA相应,DMA传输,DMA结束
    DMA技术的出现,使得外围设备可以通过DMA控制器直接访问内存,那么要考虑的一个问题就是,DMA控制器与CPU如何分时使用内存,主要有以下三种方法:
    a> 停止cpu访问内存,由DMA控制器给cpu发送一个停止新号,cpu放弃对地址总线,数据总线等相关总线的使用权,数据传输结束后,再把控制权还给cpu
    b> 周期挪用,当I/O设备没有DMA请求时,cpu按程序要求访问内存,当有DMA请求时,由I/O设备挪用一个或几个内存周期,当此时cpu不需要访问内存时,I/O设备挪用
        一两个内存周期对cpu没有影响;当cpu此时也在执行访问内存指令时,会发生内存访问冲突,但这种情况下,I/O设备的访问内存优先,这意味着在cpu访问内存
        的过程中插入DMA请求
    c> DMA与cpu交替访问内存,如果cpu的工作周期比内存存取周期长,此时采用交替访问内存的方式可以使得DMA拷贝和cpu同时达到最高效率,比如:cpu的工作周期为
       1.2us,内存存取周期小于0.6us,那么一个cpu周期可分为C1,C2两个周期,其中C1专供DMA控制器访问内存,C2专供cpu访问内存,这种方式不需要总线使用权的
       申请,建立和归还,总线使用权是通过C1,C2分时制,cpu和DMA控制器分别有自己的访问内存地址寄存器,数据寄存器和读写新号控制器,在C1周期中,如果DMA有
       访问内存请求,可将地址和数据等新号放到总线上,在C2周期中,如果cpu有访问内存请求,同样将地址,数据等新号放到总线上
    注: 常规的数据拷贝需要cpu参与,其流程是先从源地址读取到cpu中,再从cpu写到目的地址(和架构也有关系,cisc架构的X86系统也有内存到内存的指令,不需要
        借助寄存器,而risc架构的ARM没有这类指令),DMA则是通过总线直接从源到目的地址
14. REST(Representational State Transfer) 表述/表征性状态迁移,是一组架构约束条件和规范,符合该规范的架构称之为RESTful架构
    表述性状态迁移/转化缺少主语,这个主语就是资源,所以准确的说法是(web)资源表述性状态迁移,在web应用中,资源是一个被引用的实体或者信息,一段文字,一张
    图片,一段音视频等等都是资源,一个资源能被引用/识别,需要一个唯一的标识,在web中就是URI(Uniform Resource Identifier,统一资源定位符),URI的设计
    应该遵循可寻址性,自描述性
    资源是信息实体,资源具体的呈现形式叫representation,比如文本可以是txt格式,也可以是html,json,xml等其它格式,URI只代表资源实体,不表示格式,严格说
    地址后面的.html后缀是可省略的,后缀只表示格式,属于representation,而URI只描述资源的位置,representation在http请求的头信息中使用Accept和
    Content-Type字段指定
    http协议是一个无状态协议,如果客户端想要操作服务器,必须通过某种手段让服务器数据和状态发生转化,而这种转化是建立在表现层上的,所以是表现/表征状态
    转化,具体到http协议,有四种操作用来实现这种转化:GET 获取资源; POST 新建/更新资源; PUT 更新资源; DELETE 删除资源
    RESTful典型设计误区: 
    URI包含动词,因为资源表示一种实体,所以应该是名词,URI不应该有动词,动词应该放在http协议,比如汇款操作
      POST /account/1/transfer/500/to/2
    正确的做法是把动词transfer变为名词transaction,资源也可以是抽象的一种服务,正确的一种示例如下:
      POST /transaction HTTP/1.1
      HOST: 127.0.0.1
      from=1&to=2&amount=500.00
    另一个设计误区是URI中加入版本号,不同的版本可以理解为资源的不同表现形式,版本号可以在accept字段中进行区分
    参考:http://www.ruanyifeng.com/blog/2011/09/restful.html
15. 逃逸分析(Escape analysis)是编译原理中分析指针动态范围的方法,是编译器执行静态代码分析之后,对内存管理进行的优化
    在c/c++函数中定义一个局部变量,然后返回该局部变量的地址(指针),一旦函数执行完毕,函数调用栈被回收,那么任何对返回的地址的引用,
    都将破坏程序的运行甚至导致程序崩溃,我们当然可以使用new或者mallloc定义局部变量,返回之后仍然可以引用,但是何时以及如何安全释放
    又是个问题,很容易导致内存泄漏,很多现代语言java/go/erlang都有垃圾回收机制,把复杂的内存管理交给编译器,简单的说逃逸分析就是
    决定一个变量是分配在堆上还是栈上,如果变量都分配在堆上,首先是分配的速度较慢; 其次不能自动回收,需要手动or借助垃圾回收机制回收,
    分配到堆上的变量越多,GC就越频繁,这将极大影响程序的性能,比如说Go的垃圾回收,三色标记和回收过程尽量使用并发操作,
    但在start gc和re-scan阶段还是需要stop_the_world,通过逃逸分析,可以尽量把不需要分配到堆上的变量直接分配到栈上,
    堆上的变量变少,一方面减少堆内存分配的开销(同时减少内存碎片),另一方面减少gc的压力,提高程序的性能,
    编译器对代码进行分析,通过动态分析变量or对象的声明周期和作用域,找到逃逸和未逃逸对象(变量or对象是否被外部函数or线程引用),
    进而绝对分配到堆或栈上,在jvm中,对未逃逸对象的优化,除了上述堆栈分配之外,还有如下两种优化方法:
    a> 同步消除,如果一个对象不存在线程逃逸,即不会被其它线程访问到,则该对象的读写就不会存在竞争,可以消除对该对象的同步锁,
       通过-XX:+EliminateLocks可以开启同步消除
    b> 标量替换,标量是指不可分割的量,比如java中的基本数据类型和reference类型,相对的一个变量可被继续分解,称为聚合量
       如果把一个对象拆散,将其成员变量恢复到基本类型来访问,就叫标量替换
       如果逃逸分析发现一个对象不会被外部访问,且该对象可以被拆散,那么经过优化之后,并不会在对上直接生成该对象,而是在栈上创建若干成员变量,
       通过-XX:+EliminateAllocations开启标量替换, -XX:+PrintEliminateAllocations查看标量替换情况
    Go编译器会在编译期考察变量的作用域,并做一系列的检查,如果作用域在编译器是可知的,那么会分配到栈上,否则分配到堆上
15. 关于编码,哈希和加密
    a> 编码, 本质上是对信息格式的转换,是将信息转换为统一的格式,方便在不同系统间传输和处理,其目的不是为了对信息进行加密,
       其本质是可逆的映射;
       哈希, 也可称为摘要,目的是为了校验信息的完整性,保证信息在传输过程中不被篡改,一般包括以下特点:输入相同时,输出保持相同; 
       输入不同时,输出基本保持相同(存在碰撞的可能性); 通过输出,不能计算输入; 对输入的修改,都会导致不同的输出; 综上可知,其属于不可逆映射,
       哈希也可以被用来给数据加密,比如之前常用MD5加密用户密码,哈希or摘要常用的算法包括md5,sha系列;
       加密, 其目的是隐藏信息,保证数据安全传输,其与编码均为可逆映射,但是加密的点不在可用性上,加密又分为对称加密和非对称加密,区别在于
       加密和解密时的密钥是否是同一个,加密信息可以通过密钥还原为原始信息,对称加密算法:DES,AES, 非对称加密算法:RSA
16. DDos(Distributed Denial of Service,分布式拒绝服务),利用网络上被攻陷的电脑作为"肉鸡"(也可以自己实现or使用第三方工具进行ip spoof,
    不过这种方式收不到服务器返回的数据),通过一定的方式组成"僵尸网络",采用一对多的控制方式,向目标系统同时提交服务请求,目的是使系统或
    网络无法提供正常服务,常见的几种攻击方式如下:
    a> ICMP flood, ICMP(Internet控制报文协议)在Internet上用于错误处理与传递控制信息,控制信息就是指网络通不通,主机是否可达,路由是否可用
       等网络本身的消息,常用的ping程序就是这个功能,实现icmp flood的前提是快速的发包速度(每秒1k+数据包)和较宽的网络带宽(事实上flood攻击,
       都需要满足上述两个前提),icmp flood攻击有以下三种方式:直接flood,前提是自己的带宽足够,同时直接攻击会暴露自己的ip; 伪造ip flood,通过
       伪造一个ip来发送icmp报文(一般是ping程序使用的ECHO请求,icmp本身有多种请求类型); 反射, 这种模式里flood不是由攻击者发出,也不是伪造ip
       发出,而是正常通信的其它服务器发出,这种方式的实现原理是把源ip设置为被攻击服务器ip,然后向多台服务器(受骗源)发送icmp报文,接收报文的
       服务器向被攻击服务器发送应答reply报文,可知反射攻击的效果取决于受骗源的效率及是否有效
    b> UDP flood, 攻击者发送大量伪造源ip的小udp包来冲击DNS/Radius服务器或流媒体视频服务器,大量的udp数据包耗尽目标服务器的处理和响应能力,
       从而导致对合法流量的拒绝服务
    c> NTP(Network Time Protocol,网络时间协议,基于UDP) flood,是一种利用网络中NTP服务器的脆弱性(无认证,不等价数据交换,udp)来进行DDos攻击
       的行为,无论是基于DNS还是NTP,最终都是基于udp协议,udp协议在正常情况下客户端发送请求包到服务器,服务器返回响应包到客户端,但是udp是面向
       无连接的,所以客户端很容易伪造发送的请求包的源ip,把源ip修改为被攻击ip,最终服务端返回的响应包被发送到被攻击的服务器,这样就形成了一次
       反射攻击,放大攻击是一次小的请求最终会收到多余请求很多倍的响应包,可以实现四两拨千斤的效果,原因在于NTP有一个monlist命令,NTP服务器响应
       monlist命令,返回与NTP服务器进行过时间同步的最后600个客户端的ip,monlist命令的响应包按照6个ip为一组进行分割,最多会返回100个响应包,
       这样反射+放大就可以以较小的代价实现对目标服务器的N倍攻击
    d> SYN flood, 这是一种利用tcp协议特点进行的攻击方式,通过发送大量的tcp连接请求,且拒绝完成第三次握手(屏蔽服务器返回的SYN+ACK报文或者
       拒绝发送第三次的ACK报文),导致服务器维护数以万计的半连接队列而耗尽资源
    e> CC(challenge collapsar,挑战黑洞)攻击,借助代理服务器或"肉鸡"生成指向目标服务器的合法请求,原理是不停的发大量请求给服务器,造成服务器
       资源耗尽,直至宕机,CC攻击一般是发送动态页面请求,这样服务器需要对多个条件进行判断甚至需要读取数据库,比如看一个帖子,服务器需要判断是
       游客还注册用户,是否有相关权限,帖子的内容有哪些,是否有评论等等,这就需要消耗一定的cpu时间,内存及访问数据库,再比如根据名字搜索好友或者
       帖子,这种请求的消耗说不定更大,而CC攻击正是利用了这一点,模拟多个用户不停的进行访问,从而造成贷款资源被严重消耗,cpu&内存飙升,主机瘫痪,
       CC攻击是针对应用层的攻击,同时具有一定的隐蔽性,尤其是使用代理进行CC攻击的时候,会发现每个用户的连接数并不高,无法针对性的对ip进行屏蔽
    f> DNS query flood, 该方法是操作大量"肉鸡"向目标DNS服务器发送大量域名解析请求,服务器接收到请求后,先查找本地缓存,查找不到便向上层DNS服
       务器递归查询域名信息,通常请求解析的域名都是随机生成的,服务器无法查询时递归向上提交解析请求,引起连锁反应,解析过程会给服务器带来较大的
       负载,据统计,一台DNS服务器所能承受的动态域名查询个数上限是9k/s,而一台普通的机器每秒可以生成几万个域名解析请求
    g> 慢速连接攻击,一般的攻击是一段时间内海量的流量和报文,但有一种攻击却反其道而行之,以慢著称,最具代表性的是rsnake发明的slowloris,
       http协议规定http request以"\r\n\r\n"结尾,表示客户端发送结束,服务端开始处理,slowloris就是利用这一点来做DDos攻击,在http请求头中
       将connection设置为keep-alive,要求web server不要断开连接,随后缓慢的每隔一段时间发送一个key-value格式的数据到服务端,
       比如:key1:val1\r\n,导致服务端认为http头部没有接受完而一直等待,这样如果攻击者使用多线程或多个"肉鸡"来做同样的操作,服务器资源很快会被
       占满,而不再接受新的请求,slowloris有各种变种,比如Post方法向服务端提交数据,content-length字段填充一个很大的值,但缓慢的一个字节一个字节
       的提交真正的数据
    参考:https://zhuanlan.zhihu.com/p/22953451
